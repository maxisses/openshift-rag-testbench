{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a331fd6",
   "metadata": {},
   "source": [
    "## RAG example with Langchain, Milvus, and vLLM\n",
    "\n",
    "Requirements:\n",
    "- A Milvus instance, either standalone or cluster.\n",
    "- Connection credentials to Milvus must be available as environment variables: MILVUS_USERNAME and MILVUS_PASSWORD.\n",
    "- A vLLM inference endpoint. In this example we use the OpenAI Compatible API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e712b3e8-f406-4387-9188-3e2f20a6841f",
   "metadata": {},
   "source": [
    "### Needed packages and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a359bd-4f69-4e88-82c0-5763c26aa0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q einops==0.7.0 langchain==0.1.9 pymilvus==2.3.6 sentence-transformers==2.4.0 openai==1.13.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e11d23-c0ad-4875-b67f-149fc8b14725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4537b",
   "metadata": {},
   "source": [
    "#### Bases parameters, Inference server and Milvus info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98401d00-bcc5-4267-8af1-04d8e9cde647",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baf1a6-4111-4b40-b43a-833438bdc222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace values according to your Milvus deployment\n",
    "\n",
    "### local / in-cluster\n",
    "# INFERENCE_SERVER_URL = \"http://vllm.rag-with-llama2-model-deployment.svc.cluster.local:8000/v1\"\n",
    "\n",
    "### ocp route\n",
    "INFERENCE_SERVER_URL =\"http://vllm:8000/v1\"\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "MAX_TOKENS=2048\n",
    "TOP_P=0.95\n",
    "TEMPERATURE=0.01\n",
    "PRESENCE_PENALTY=1.03\n",
    "MILVUS_HOST = \"vectordb-milvus\"\n",
    "MILVUS_PORT = 19530\n",
    "MILVUS_USERNAME = \"root\"\n",
    "MILVUS_PASSWORD = \"Milvus\"\n",
    "MILVUS_COLLECTION = \"redhat_notes\"\n",
    "RAG_NEAREST_NEIGHBOURS = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c1b1a",
   "metadata": {},
   "source": [
    "#### Initialize the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6a3e3-5ccd-441e-b80d-427555d9e9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_kwargs = {'trust_remote_code': True}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "store = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT, \"user\": MILVUS_USERNAME, \"password\": MILVUS_PASSWORD},\n",
    "    collection_name=MILVUS_COLLECTION,\n",
    "    metadata_field=\"metadata\",\n",
    "    text_field=\"page_content\",\n",
    "    drop_old=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72a3a2b",
   "metadata": {},
   "source": [
    "#### Initialize query chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948603a-2c64-4bff-8ef2-161737c96157",
   "metadata": {},
   "outputs": [],
   "source": [
    "template=\"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant named SnemeisBot answering questions.\n",
    "You will be given a question you need to answer, and a context about everything a sales team wrote down about its customers to provide you with information. You must answer the question based as much as possible on this context.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: {question} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7892e37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8fd396-0798-45c5-8969-6b6ede134c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "            You are a helpful, respectful and honest assistant named SnemeisBot. You are answering a question.\n",
    "            You will be given a question you need to answer, and a context about everything a sales team wrote down about its customers to provide you with information. You must answer the question based as much as possible on this context.\n",
    "\n",
    "            If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, don't share false information.\n",
    "            <</SYS>>\n",
    "            \n",
    "            Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question in a helpful, respectful manner:\n",
    "            ------\n",
    "            <ctx>\n",
    "            {context}\n",
    "            </ctx>\n",
    "            ------\n",
    "            <hs>\n",
    "            {history}\n",
    "            </hs>\n",
    "            ------\n",
    "            {question}\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "\n",
    "# QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "prompt_template = PromptTemplate(\n",
    "        input_variables=[\"history\", \"context\", \"question\"],\n",
    "        template=template,\n",
    "    )\n",
    "\n",
    "llm = VLLMOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    openai_api_base=INFERENCE_SERVER_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    top_p=TOP_P,\n",
    "    temperature=TEMPERATURE,\n",
    "    presence_penalty=PRESENCE_PENALTY,\n",
    "    streaming=True,\n",
    "    verbose=False,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": int(RAG_NEAREST_NEIGHBOURS)}),\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": prompt_template, \n",
    "            \"verbose\": True,\n",
    "            \"memory\": ConversationBufferMemory(\n",
    "                memory_key=\"history\",\n",
    "                input_key=\"question\"),},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45ad23",
   "metadata": {},
   "source": [
    "#### Query example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d2fd1-f36c-409d-8e52-ec6d23a56ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"In that context - who is the customer?\"\n",
    "result = qa_chain.invoke({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9556720-25e3-4f9e-aecf-61cee0287c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d75d0c",
   "metadata": {},
   "source": [
    "#### Retrieve source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acda357e-558a-4879-8ad8-21f0567f2f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list_doctitle = []\n",
    "    unique_list_doccontent = []\n",
    "    for item in input_list:\n",
    "        if item.metadata['source'] not in unique_list_doctitle:\n",
    "            unique_list_doctitle.append(item.metadata['source'])\n",
    "            unique_list_doccontent.append(item.page_content)\n",
    "    return unique_list_doctitle, unique_list_doccontent\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f0f15-0491-41d5-87e4-974bad943dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_list):\n",
    "    unique_list = []\n",
    "    for item in input_list:\n",
    "        if item not in unique_list:\n",
    "            print(item.page_content)\n",
    "            unique_list.append(item.metadata['source'])\n",
    "    return unique_list\n",
    "\n",
    "results = remove_duplicates(result['source_documents'])\n",
    "\n",
    "for s in results:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720bbea-d228-4211-8144-8286934ff70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
